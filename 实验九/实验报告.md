# 姓名

- 倪诗宇

# 学号

- 201900180065

# 实验日期

- 2021.11.21

# 实验题目

- 了解cv::matchTemplate函数的用法，并选择合适的测试图像进行测试，要求：

  - 理解TM_SQDIFF等相似性度量方法的含义和适用情况。

  - 针对模板与图像目标存在颜色（亮度）差异、几何形变等情况进行测试分析，可以重点对比TM_SQDIFF和TM_CCOEFF_NORMED进行对比。

# 实验过程中遇到的问题和解决办法

问题主要就是一开始不会调用，也不明白这些匹配方式是如何实现的，因此有了下面的过程

# 函数讲解

- [opencv](https://so.csdn.net/so/search?from=pc_blog_highlight&q=opencv)实现了一部分通过模板与目标图像进行寻找最佳匹配的方面matchTemplat() , 函数声明如下所示

```c++
void cv::matchTemplate(
    cv::InputArray image, // 用于搜索的输入图像
    cv::InputArray teml, // 用于匹配的模板，和image类型相同
    cv::OutputArray result, // 匹配结果图像, 类型 32F
    int method // 用于比较的方法，opencv提供6种
    }
```

- 函数调用方式为
  - source_image是原图
  - template_image是选取的需要匹配的特征图
  - target_image是存储匹配值的结果图

```c++
matchTemplate(source_image, template_image, target_image, TM_CCOEFF_NORMED);//匹配函数调用
double minVal, maxVal; //用于存储匹配值的最大值和最小值
Point minLoc, maxLoc; //用于存储匹配最大值的位置和最小值的位置

//寻找最佳匹配位置
minMaxLoc(target_image, &minVal, &maxVal, &minLoc, &maxLoc);
//画圈标出匹配点，这里使用maxLoc还是minLoc和选择的匹配method有关
circle(source_image, Point(maxLoc.x + template_image.cols / 2, maxLoc.y + template_image.rows / 2), 10, Scalar(0, 0, 255), 2, 8, 0);
```

# 我们将尝试用不同方式来匹配图片中，人手中的黄色物品

<center>
    <img src="C:\Users\nishiyu\AppData\Roaming\Typora\typora-user-images\image-20211121205112690.png" width="300"/>
</center>

# opencv支持的比较方法有6种

- TM_SQDIFF：该方法使用平方差进行匹配，因此最佳的匹配结果在结果为0处，值越大匹配结果越差。

  - 可以看出的是，若分别改变template图片和source图片的亮度，匹配效果将会变差

  ![image-20211121211244043](C:\Users\nishiyu\AppData\Roaming\Typora\typora-user-images\image-20211121211244043.png)

  - 该方法使用基本的平方差，就是将要匹配的template图片放在一张图上扫描，最好的匹配结果就是0

  <center class="half">
      <img src="C:\Users\nishiyu\AppData\Roaming\Typora\typora-user-images\image-20211121204922439.png" width="300"/>
      <img src="C:\Users\nishiyu\AppData\Roaming\Typora\typora-user-images\image-20211121210156891.png" width="300"/>
  </center>

  - 右图是将匹配矩阵中的值映射到0 ---- 1之间显示的，因为需要使用 imshow 对其进行显示
  - 可以看到，左图显示，用该方法能够较好实现匹配。匹配的值越大，图像越亮。从右图中，我们可以看到匹配结果，手中物品匹配结果呈黑色，因此差距很小。

- TM_SQDIFF_NORMED：该方法使用归一化的平方差进行匹配，最佳匹配也在结果为0处。

  - 这个公式中可以将 $T , I$ 的每个值排列起来看作一个向量，公式可以转化为  $R = \frac{|T-I|^2}{|T|\,|I|}$ 

    ![image-20211121211222850](C:\Users\nishiyu\AppData\Roaming\Typora\typora-user-images\image-20211121211222850.png)

    <center class="half">
        <img src="C:\Users\nishiyu\AppData\Roaming\Typora\typora-user-images\image-20211121210413041.png" width="300"/>
        <img src="C:\Users\nishiyu\AppData\Roaming\Typora\typora-user-images\image-20211121210431850.png" width="300"/>
    </center>

  - 因为匹配的结果矩阵已经进行了归一化，因此我们可以直接进行显示

  - 可以看到，这种方法也能很好实现匹配

  - ==这种标准化操作可以保证当模板和图像各个像素的亮度都乘上了同一个系数时，相关度不发生变化。也就是说当 $I(x,y)$ 和 $T(x,y)$ 变为 $k×I(x,y)$ 和 $k×T(x,y)$ 时，$R(x,y)$ 不发生变化==

- TM_CCORR：相关性匹配方法，该方法使用源图像与模板图像的卷积结果进行匹配，因此，最佳匹配位置在值最大处，值越小匹配结果越差。

  - 这个公式中可以将 $T , I$ 的每个值排列起来看作一个向量，公式可以转化为  $R = T \cdot I$。==我认为这并不能用来代表相似性，因为亮度大的区域，得分自然就高==

  ![image-20211121213118276](C:\Users\nishiyu\AppData\Roaming\Typora\typora-user-images\image-20211121213118276.png)

  <center class="half">
      <img src="C:\Users\nishiyu\AppData\Roaming\Typora\typora-user-images\image-20211121213728195.png" width="300"/>
      <img src="C:\Users\nishiyu\AppData\Roaming\Typora\typora-user-images\image-20211121213737336.png" width="300"/>
  </center>

  - 右图是将匹配的结果矩阵归一化的显示
  - 由左图看出，该方法匹配并不准确，从公式上看，该匹配方法容易受到亮度的影响
  - 原图中，左边比右边更亮。可以看出，匹配结果中，左边的匹配值也比右边大

- TM_CCORR_NORMED：归一化的相关性匹配方法，与相关性匹配方法类似，最佳匹配位置也是在值最大处。

  - 这个公式中可以将 $T , I$ 的每个值排列起来看作一个向量，公式可以转化为  $R = \frac{T \cdot I}{|T||I|\\} = \cos(T\,,I)$。==这里的公式就是求$T$ 与 $I$ 的 $\cos$ ， 其实也就是两个向量的相似度，可以用作图像匹配的标准==

  ![image-20211121215859926](C:\Users\nishiyu\AppData\Roaming\Typora\typora-user-images\image-20211121215859926.png)

  <center class="half">
      <img src="C:\Users\nishiyu\AppData\Roaming\Typora\typora-user-images\image-20211121220206661.png" width="300"/>
      <img src="C:\Users\nishiyu\AppData\Roaming\Typora\typora-user-images\image-20211121220216721.png" width="300"/>
  </center>

  - 结果可知，该方法能够较好得实现图像匹配
  - ==这个方法和 标准化差值平方和匹配 类似，都是去除了亮度线性变化对相似度计算的影响。可以保证图像和模板同时变亮或变暗k倍时结果不变。==

- TM_CCOEFF：相关性系数匹配方法，该方法使用源图像与其均值的差、模板与其均值的差二者之间的相关性进行匹配。

  ![image-20211121230050907](C:\Users\nishiyu\AppData\Roaming\Typora\typora-user-images\image-20211121230050907.png)

  <center class="half">
      <img src="C:\Users\nishiyu\AppData\Roaming\Typora\typora-user-images\image-20211121224325200.png" width="300"/>
      <img src="C:\Users\nishiyu\AppData\Roaming\Typora\typora-user-images\image-20211121224341192.png" width="300"/>
  </center>

  - 由匹配结果图片可以看出，人手中的东西是一个白点，证明匹配效果最好
  - 从左图中我们也可以看出，匹配结果较好

- TM_CCOEFF_NORMED：在减去了各自的平均值之外，还要各自除以各自的方差。经过减去平均值和除以方差这么两步操作之后，无论是我们的待检图像还是模板都被标准化了，这样可以==保证图像和模板分别改变光照亮不影响计算结果==。计算出的相关系数被限制在了 -1 到 1 之间，1 表示完全相同，-1 表示两幅图像的亮度正好相反，0 表示两幅图像之间没有线性关系。

  - 也可以看做利用 $\cos$ 求相似度 

  ![image-20211121225301172](C:\Users\nishiyu\AppData\Roaming\Typora\typora-user-images\image-20211121225301172.png)

  <center class="half">
      <img src="C:\Users\nishiyu\AppData\Roaming\Typora\typora-user-images\image-20211121225359857.png" width="300"/>
      <img src="C:\Users\nishiyu\AppData\Roaming\Typora\typora-user-images\image-20211121225406285.png" width="300"/>
  </center>

  - 由匹配结果图片可以看出，人手中的东西是一个白点，证明匹配效果最好
- 从左图中我们也可以看出，匹配结果较好

# 结论分析与体会

### 对比 TM_SQDIFF 和 TM_CCOEFF_NORMED

- 对template图片的亮度进行改变

  三通道像素值均增加 80

  - TM_CCOEFF_NORMED匹配结果
    - 可以看到，匹配结果依然正确
    - 通过匹配结果也就是右图可以看出，匹配的结果和template图像亮度增加之前无明显变化

  <center class="half">
      <img src="C:\Users\nishiyu\AppData\Roaming\Typora\typora-user-images\image-20211122132358631.png" width="300"/>
      <img src="C:\Users\nishiyu\AppData\Roaming\Typora\typora-user-images\image-20211122132412390.png" width="300"/>
  </center>

  - TM_SQDIFF匹配结果

    - 可以发现的是，当我们增大template图片的亮度后，最好的匹配结果落在了地上
    - 从匹配结果图，也就是右图中我们可以看出，图像中右边的树林匹配结果不好，左边的底面匹配结果都不错
    - 这是很容易理解的。因为本放大使用的是平方差，因此当增大匹配图像亮度后，匹配的结果会更倾向于寻找亮度更大的区域，因为这样的区域计算出来的平方差才比较小。

    <center class="half">
        <img src="C:\Users\nishiyu\AppData\Roaming\Typora\typora-user-images\image-20211122132916938.png" width="300"/>
        <img src="C:\Users\nishiyu\AppData\Roaming\Typora\typora-user-images\image-20211122132922462.png" width="300"/>
    </center>

- 对template图片的大小进行调整（变大）

  - TM_CCOEFF_NORMED匹配结果

    - 从右图可以看出，匹配的结果较图像大小调整之前有明显变化
    - 匹配结果略有下降
    - 当匹配图像大小调整后，在匹配过程中，检测框的大小发生了变化，框中的像素发生了变化，因此结果也会发生变化

    <center class="half">
        <img src="C:\Users\nishiyu\AppData\Roaming\Typora\typora-user-images\image-20211122134903605.png" width="300"/>
        <img src="C:\Users\nishiyu\AppData\Roaming\Typora\typora-user-images\image-20211122134908773.png" width="300"/>
    </center>

  - TM_SQDIFF匹配结果

    - 从右图可以看出，匹配结果较图像大小调整之前有明显变化
    - 匹配结果略有下降
    - 当匹配图像大小调整后，在匹配过程中，检测框的大小发生了变化，框中的像素发生了变化，因此结果也会发生变化

    <center class="half">
        <img src="C:\Users\nishiyu\AppData\Roaming\Typora\typora-user-images\image-20211122135322675.png" width="300"/>
        <img src="C:\Users\nishiyu\AppData\Roaming\Typora\typora-user-images\image-20211122135328114.png" width="300"/>
    </center>

    ```c++
    #include <opencv2/core/core.hpp>
    #include<opencv2/highgui/highgui.hpp>
    #include"opencv2/imgproc/imgproc.hpp"
    #include"opencv2/opencv.hpp"
    using namespace std;
    using namespace cv;
    Mat source_image;
    Mat template_image;
    Mat target_image;
    
    int main()
    {
        source_image = imread("D:\\cLion\\project\\02.jpg");
        template_image = imread("D:\\cLion\\project\\02_temp.jpg");
    //调整大小时使用
    //    for(int i = 0 ; i < template_image.rows ; i++)
    //        for(int j = 0 ; j < template_image.cols ; j++)
    //            for(int c = 0 ; c < 3 ; c++)
    //                template_image.at<Vec3b>(i,j)[c] = saturate_cast<uchar>(template_image.at<Vec3b>(i,j)[c] + 80);
    
        matchTemplate(source_image, template_image, target_image, TM_SQDIFF);
    
    
        double minVal, maxVal; //存储匹配的最大值和最小值
        Point minLoc, maxLoc; //存储匹配最大值的位置和最小值的位置0
        //寻找最佳匹配位置
        minMaxLoc(target_image, &minVal, &maxVal, &minLoc, &maxLoc);
        //Point中使用max或min根据具体方法确定
        circle(source_image, Point(minLoc.x + template_image.cols / 2, minLoc.y + template_image.rows / 2), 10, Scalar(0, 0, 255), 2, 8, 0);
    
        normalize(target_image, target_image, 0, 1, NORM_MINMAX);
        imshow("Source Window" , source_image);
        imshow("Template Window" , template_image);
        imshow("Target Window" , target_image);
        waitKey(0);
    }
    ```

    

    

    

  

  
